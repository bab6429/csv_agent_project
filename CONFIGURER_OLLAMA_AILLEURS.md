# üîß Configurer Ollama install√© ailleurs

Si Ollama est install√© dans un autre emplacement que votre projet, voici comment le configurer.

## üìç Deux choses √† configurer

### 1. La commande `ollama` (pour les scripts)
### 2. L'URL de l'API Ollama (pour l'application Python)

---

## üîç √âtape 1 : Trouver o√π Ollama est install√©

### M√©thode 1 : Chercher dans le menu D√©marrer
1. Cliquez sur D√©marrer
2. Cherchez "Ollama"
3. Clic droit ‚Üí "Ouvrir l'emplacement du fichier"
4. Notez le chemin (ex: `C:\Users\VotreNom\AppData\Local\Programs\Ollama`)

### M√©thode 2 : Chercher manuellement
Ollama est g√©n√©ralement install√© dans :
- `C:\Users\VotreNom\AppData\Local\Programs\Ollama`
- `C:\Program Files\Ollama`
- Ou un autre emplacement personnalis√©

### M√©thode 3 : V√©rifier si Ollama tourne
Ouvrez un navigateur et allez sur : http://localhost:11434/api/tags

Si vous voyez une r√©ponse JSON, Ollama tourne et l'API est accessible ! ‚úÖ

---

## ‚öôÔ∏è √âtape 2 : Configurer l'URL de l'API (IMPORTANT)

C'est la partie la plus importante ! M√™me si la commande `ollama` n'est pas dans le PATH, l'application Python peut utiliser Ollama via son API HTTP.

### Option A : Ollama tourne sur localhost (par d√©faut)

Cr√©ez un fichier `.env` dans votre projet avec :

```env
OLLAMA_MODEL_NAME=phi3
OLLAMA_BASE_URL=http://localhost:11434
```

### Option B : Ollama tourne sur un autre port

Si Ollama tourne sur un autre port (par exemple 11435), modifiez :

```env
OLLAMA_MODEL_NAME=phi3
OLLAMA_BASE_URL=http://localhost:11435
```

### Option C : Ollama tourne sur un autre serveur

Si Ollama tourne sur une autre machine (ex: 192.168.1.100), modifiez :

```env
OLLAMA_MODEL_NAME=phi3
OLLAMA_BASE_URL=http://192.168.1.100:11434
```

---

## üõ†Ô∏è √âtape 3 : Ajouter Ollama au PATH (optionnel)

Si vous voulez utiliser la commande `ollama` dans les scripts, ajoutez-le au PATH :

### M√©thode Windows (via l'interface)

1. **Trouvez le chemin d'installation d'Ollama** (voir √âtape 1)
2. **Ouvrez les Variables d'environnement** :
   - Appuyez sur `Windows + R`
   - Tapez `sysdm.cpl` et Entr√©e
   - Onglet "Avanc√©" ‚Üí "Variables d'environnement"
3. **Modifiez la variable PATH** :
   - Dans "Variables syst√®me", trouvez "Path"
   - Cliquez sur "Modifier"
   - Cliquez sur "Nouveau"
   - Ajoutez le chemin vers Ollama (ex: `C:\Users\VotreNom\AppData\Local\Programs\Ollama`)
   - Cliquez sur "OK" partout
4. **Red√©marrez votre terminal** pour que les changements prennent effet

### M√©thode PowerShell (temporaire, pour la session actuelle)

```powershell
$env:Path += ";C:\Users\VotreNom\AppData\Local\Programs\Ollama"
```

(Remplacez par votre chemin r√©el)

---

## ‚úÖ √âtape 4 : V√©rifier la configuration

### Test 1 : V√©rifier que l'API Ollama est accessible

Ouvrez PowerShell et tapez :

```powershell
python test_ollama.py
```

Ce script va :
- ‚úÖ V√©rifier que l'API Ollama est accessible
- ‚úÖ V√©rifier que le mod√®le est disponible
- ‚úÖ Tester une g√©n√©ration

### Test 2 : Test manuel de l'API

```powershell
# Test simple avec PowerShell
Invoke-WebRequest -Uri "http://localhost:11434/api/tags" | Select-Object -ExpandProperty Content
```

Vous devriez voir une liste de mod√®les en JSON.

---

## üéØ Configuration rapide (sans modifier le PATH)

Si vous ne voulez pas modifier le PATH, voici la solution la plus simple :

### 1. Cr√©ez le fichier `.env` manuellement

Dans votre projet (`C:\Users\halca\csv_agent_project`), cr√©ez un fichier `.env` avec :

```env
OLLAMA_MODEL_NAME=phi3
OLLAMA_BASE_URL=http://localhost:11434
```

### 2. T√©l√©chargez le mod√®le manuellement

Si vous avez acc√®s √† Ollama (via l'interface ou un autre terminal), t√©l√©chargez le mod√®le :

```bash
# Depuis n'importe quel terminal o√π ollama fonctionne
ollama pull phi3
```

### 3. Testez

```bash
python test_ollama.py
```

---

## üêõ D√©pannage

### Erreur : "Connection refused" ou "Cannot connect"

**Probl√®me** : Ollama n'est pas d√©marr√© ou l'URL est incorrecte.

**Solution** :
1. V√©rifiez que Ollama tourne (ouvrez l'application Ollama)
2. V√©rifiez l'URL dans `.env` : `OLLAMA_BASE_URL=http://localhost:11434`
3. Testez dans le navigateur : http://localhost:11434/api/tags

### Erreur : "Model not found"

**Probl√®me** : Le mod√®le n'est pas t√©l√©charg√©.

**Solution** :
1. T√©l√©chargez le mod√®le : `ollama pull phi3`
2. V√©rifiez les mod√®les disponibles : `ollama list`
3. Mettez √† jour `OLLAMA_MODEL_NAME` dans `.env`

### La commande `ollama` ne fonctionne pas

**Probl√®me** : Ollama n'est pas dans le PATH.

**Solution** :
- **Option 1** : Utilisez l'URL de l'API directement (voir ci-dessus)
- **Option 2** : Ajoutez Ollama au PATH (voir √âtape 3)
- **Option 3** : Utilisez le chemin complet : `C:\chemin\vers\ollama.exe pull phi3`

---

## üìù R√©sum√©

**Pour utiliser Ollama avec votre projet, vous avez besoin de :**

1. ‚úÖ **Ollama install√© et d√©marr√©** (peu importe o√π)
2. ‚úÖ **Fichier `.env` avec l'URL correcte** :
   ```env
   OLLAMA_MODEL_NAME=phi3
   OLLAMA_BASE_URL=http://localhost:11434
   ```
3. ‚úÖ **Mod√®le t√©l√©charg√©** (via `ollama pull phi3` ou l'interface)

**La commande `ollama` dans le PATH est optionnelle** - l'application Python utilise l'API HTTP, pas la commande en ligne !

---

## üöÄ Test final

Une fois configur√©, testez avec :

```bash
python test_ollama.py
```

Si tous les tests passent, vous pouvez lancer l'application :

```bash
streamlit run app.py
```

