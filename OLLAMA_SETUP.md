# ü¶ô Configuration Ollama pour l'Agent CSV

Ce projet supporte maintenant **Ollama** pour utiliser des LLM en local, avec un fallback automatique vers **Google Gemini** si Ollama n'est pas disponible.

## üìã Pr√©requis

1. **Installer Ollama** : https://ollama.com/download
2. **T√©l√©charger un mod√®le** :
   ```bash
   ollama pull llama3.2
   ```
   Autres mod√®les recommand√©s :
   - `ollama pull mistral` - Bon √©quilibre performance/taille
   - `ollama pull phi3` - Tr√®s l√©ger (~2GB)
   - `ollama pull codellama` - Sp√©cialis√© pour le code

## ‚öôÔ∏è Configuration

### Variables d'environnement

Cr√©ez un fichier `.env` √† la racine du projet avec :

```env
# Mod√®le Ollama √† utiliser (par d√©faut: llama3.2)
OLLAMA_MODEL_NAME=llama3.2

# URL d'Ollama (par d√©faut: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Cl√© API Google Gemini (optionnelle si Ollama est install√©)
GOOGLE_API_KEY=your_google_api_key_here

# Forcer un provider sp√©cifique (optionnel)
# USE_OLLAMA=true  # Force Ollama
# USE_GEMINI=true  # Force Gemini
```

### Comportement par d√©faut

1. **Auto-d√©tection** : Le syst√®me essaie d'abord Ollama
2. **Si Ollama est disponible** : Utilise le mod√®le Ollama configur√©
3. **Si Ollama n'est pas disponible** : Fallback automatique vers Gemini (n√©cessite `GOOGLE_API_KEY`)

## üöÄ Utilisation

### Installation des d√©pendances

```bash
pip install -r requirements.txt
```

### D√©marrer Ollama

Assurez-vous qu'Ollama est en cours d'ex√©cution :

```bash
# V√©rifier qu'Ollama fonctionne
ollama list

# Si Ollama n'est pas d√©marr√©, il se lancera automatiquement
# Sinon, d√©marrez-le manuellement selon votre OS
```

### Lancer l'application

```bash
# Interface Streamlit
streamlit run app.py

# Ou en ligne de commande
python csv_agent.py fichier.csv
```

## üîß Forcer un provider sp√©cifique

### Forcer Ollama

```bash
export USE_OLLAMA=true
python csv_agent.py fichier.csv
```

### Forcer Gemini

```bash
export USE_GEMINI=true
python csv_agent.py fichier.csv
```

## üìä Avantages d'Ollama

- ‚úÖ **Gratuit** : Pas de limite de requ√™tes
- ‚úÖ **Local** : Donn√©es restent sur votre machine (confidentialit√©)
- ‚úÖ **Hors ligne** : Fonctionne sans connexion internet
- ‚úÖ **Pas de cl√© API** : Pas besoin de configurer une cl√© API

## ‚ö†Ô∏è Limitations

- ‚ö†Ô∏è **Ressources** : N√©cessite de la RAM (4-8GB recommand√©s selon le mod√®le)
- ‚ö†Ô∏è **Performance** : G√©n√©ralement plus lent que les API cloud
- ‚ö†Ô∏è **Qualit√©** : Varie selon le mod√®le choisi

## üéØ Recommandations de mod√®les

Pour un agent CSV, nous recommandons :

- **llama3.2** : Bon √©quilibre performance/taille (~2GB)
- **mistral** : Excellente qualit√©, un peu plus lourd
- **phi3** : Tr√®s l√©ger, bon pour les machines avec peu de RAM
- **codellama** : Optimis√© pour g√©n√©rer du code Python

## üêõ D√©pannage

### Ollama n'est pas d√©tect√©

1. V√©rifiez qu'Ollama est install√© : `ollama --version`
2. V√©rifiez qu'Ollama tourne : `ollama list`
3. V√©rifiez l'URL dans `.env` : `OLLAMA_BASE_URL=http://localhost:11434`

### Erreur "Connection refused"

- Assurez-vous qu'Ollama est d√©marr√©
- V√©rifiez que le port 11434 n'est pas bloqu√© par un firewall

### Le mod√®le n'existe pas

- T√©l√©chargez le mod√®le : `ollama pull llama3.2`
- V√©rifiez les mod√®les disponibles : `ollama list`
- Mettez √† jour `OLLAMA_MODEL_NAME` dans `.env`

