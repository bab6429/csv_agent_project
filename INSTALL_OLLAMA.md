# ü¶ô Guide d'installation Ollama - Mod√®les l√©gers (‚â§7B)

## üì• √âtape 1 : Installer Ollama

### Windows
1. T√©l√©chargez l'installateur depuis : https://ollama.com/download/windows
2. Ex√©cutez l'installateur (OllamaSetup.exe)
3. Ollama se lancera automatiquement apr√®s l'installation

### V√©rifier l'installation
Ouvrez un terminal (PowerShell ou CMD) et tapez :
```bash
ollama --version
```

Si vous voyez un num√©ro de version, c'est bon ! ‚úÖ

---

## üéØ √âtape 2 : Choisir un mod√®le l√©ger (‚â§7B)

Pour un PC peu puissant, voici les meilleurs mod√®les recommand√©s :

### ‚≠ê **Recommandation #1 : Phi-3 Mini (3.8B)**
- **Taille** : ~2.3 GB
- **RAM n√©cessaire** : ~4-6 GB
- **Qualit√©** : Excellente pour sa taille
- **Vitesse** : Tr√®s rapide
- **Commande** : `ollama pull phi3`

### ‚≠ê **Recommandation #2 : Llama 3.2 (3B)**
- **Taille** : ~2.0 GB
- **RAM n√©cessaire** : ~4-6 GB
- **Qualit√©** : Tr√®s bonne
- **Vitesse** : Rapide
- **Commande** : `ollama pull llama3.2`

### ‚≠ê **Recommandation #3 : Mistral 7B**
- **Taille** : ~4.1 GB
- **RAM n√©cessaire** : ~8 GB
- **Qualit√©** : Excellente
- **Vitesse** : Moyenne
- **Commande** : `ollama pull mistral`

### Autres options l√©g√®res :
- **Gemma 2B** : `ollama pull gemma2:2b` (~1.4 GB)
- **TinyLlama 1.1B** : `ollama pull tinyllama` (~637 MB) - Tr√®s rapide mais qualit√© limit√©e

---

## üì¶ √âtape 3 : T√©l√©charger le mod√®le

Ouvrez un terminal et ex√©cutez :

```bash
# Pour Phi-3 Mini (recommand√© pour PC peu puissant)
ollama pull phi3

# OU pour Llama 3.2 (alternative)
ollama pull llama3.2
```

Le t√©l√©chargement peut prendre quelques minutes selon votre connexion internet.

### V√©rifier les mod√®les t√©l√©charg√©s
```bash
ollama list
```

Vous devriez voir votre mod√®le dans la liste.

---

## üöÄ √âtape 4 : Tester le mod√®le

### Test rapide en ligne de commande
```bash
ollama run phi3
# Ou
ollama run llama3.2
```

Tapez une question et appuyez sur Entr√©e. Tapez `/bye` pour quitter.

### Test avec Python
```python
import requests

response = requests.post(
    'http://localhost:11434/api/generate',
    json={
        'model': 'phi3',  # ou 'llama3.2'
        'prompt': 'Bonjour, peux-tu te pr√©senter ?',
        'stream': False
    }
)
print(response.json()['response'])
```

---

## ‚öôÔ∏è √âtape 5 : Configurer votre projet

### Option A : Via fichier .env
Cr√©ez un fichier `.env` √† la racine du projet :

```env
# Utiliser Phi-3 Mini
OLLAMA_MODEL_NAME=phi3

# Ou utiliser Llama 3.2
# OLLAMA_MODEL_NAME=llama3.2

# URL par d√©faut (ne changez que si n√©cessaire)
OLLAMA_BASE_URL=http://localhost:11434
```

### Option B : Via variable d'environnement syst√®me
```bash
# Windows PowerShell
$env:OLLAMA_MODEL_NAME="phi3"

# Windows CMD
set OLLAMA_MODEL_NAME=phi3
```

---

## üéÆ √âtape 6 : Lancer votre application

```bash
# Installer les d√©pendances (si pas d√©j√† fait)
pip install -r requirements.txt

# Lancer l'application Streamlit
streamlit run app.py

# OU lancer en ligne de commande
python csv_agent.py votre_fichier.csv
```

L'application d√©tectera automatiquement Ollama et utilisera votre mod√®le local ! üéâ

---

## üí° Optimisations pour PC peu puissant

### 1. R√©duire le nombre de threads CPU
Ollama utilise tous les c≈ìurs CPU par d√©faut. Vous pouvez limiter :

**Windows** : Modifier les variables d'environnement syst√®me
```bash
# PowerShell
$env:OLLAMA_NUM_THREAD="4"  # Utilise 4 threads au lieu de tous
```

### 2. Utiliser un mod√®le quantifi√© (plus l√©ger)
Certains mod√®les ont des versions quantifi√©es plus l√©g√®res :
```bash
# Exemple avec Llama 3.2 quantifi√© (si disponible)
ollama pull llama3.2:q4_0  # Version quantifi√©e 4-bit
```

### 3. Fermer les autres applications
Lib√©rez de la RAM en fermant les applications inutiles.

### 4. V√©rifier la RAM disponible
```bash
# Windows PowerShell
Get-CimInstance Win32_OperatingSystem | Select-Object TotalVisibleMemorySize, FreePhysicalMemory
```

**Recommandations RAM** :
- Phi-3 / Llama 3.2 : Minimum 4 GB RAM libre
- Mistral 7B : Minimum 8 GB RAM libre

---

## üêõ D√©pannage

### Ollama ne d√©marre pas
```bash
# V√©rifier si Ollama tourne
ollama list

# Si erreur, red√©marrer Ollama
# Windows : Cherchez "Ollama" dans le menu D√©marrer et relancez
```

### Le mod√®le est trop lent
- Essayez un mod√®le plus petit (Phi-3 au lieu de Mistral)
- R√©duisez le nombre de threads CPU
- Fermez les autres applications

### Erreur "out of memory"
- Utilisez un mod√®le plus petit
- Fermez les autres applications
- Red√©marrez votre PC pour lib√©rer la RAM

### Le mod√®le n'est pas trouv√©
```bash
# V√©rifier les mod√®les install√©s
ollama list

# Si le mod√®le n'est pas l√†, t√©l√©chargez-le
ollama pull phi3
```

---

## üìä Comparaison des mod√®les l√©gers

| Mod√®le | Taille | RAM min | Vitesse | Qualit√© | Recommandation |
|--------|--------|---------|---------|---------|----------------|
| **Phi-3 Mini** | 2.3 GB | 4 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ **Meilleur choix** |
| **Llama 3.2** | 2.0 GB | 4 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ Excellent |
| **Gemma 2B** | 1.4 GB | 3 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚úÖ Tr√®s l√©ger |
| **Mistral 7B** | 4.1 GB | 8 GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è Si vous avez 8GB+ RAM |
| **TinyLlama** | 637 MB | 2 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚ö†Ô∏è Qualit√© limit√©e |

---

## ‚úÖ Checklist de d√©marrage

- [ ] Ollama install√© et fonctionnel (`ollama --version`)
- [ ] Mod√®le t√©l√©charg√© (`ollama pull phi3` ou `llama3.2`)
- [ ] Mod√®le test√© (`ollama run phi3`)
- [ ] Fichier `.env` cr√©√© avec `OLLAMA_MODEL_NAME=phi3`
- [ ] D√©pendances install√©es (`pip install -r requirements.txt`)
- [ ] Application lanc√©e (`streamlit run app.py`)

---

## üéØ Ma recommandation finale

Pour un PC peu puissant, je recommande **Phi-3 Mini** :

```bash
ollama pull phi3
```

Puis dans votre `.env` :
```env
OLLAMA_MODEL_NAME=phi3
```

C'est le meilleur compromis entre taille, vitesse et qualit√© pour un PC avec peu de ressources ! üöÄ

