"""
Factory pour cr√©er des instances LLM avec support Ollama et fallback Gemini
"""
import os
import requests
from typing import Optional
from langchain_core.language_models import BaseChatModel
from config import Config


def _test_ollama_connection(base_url: str = None, timeout: int = 2) -> bool:
    """
    Teste si Ollama est disponible et accessible
    
    Args:
        base_url: URL de base d'Ollama (par d√©faut http://localhost:11434)
        timeout: Timeout en secondes pour le test de connexion
        
    Returns:
        True si Ollama est accessible, False sinon
    """
    if base_url is None:
        base_url = Config.OLLAMA_BASE_URL
    
    try:
        # Test simple de connexion √† l'API Ollama
        response = requests.get(f"{base_url}/api/tags", timeout=timeout)
        return response.status_code == 200
    except (requests.exceptions.RequestException, requests.exceptions.Timeout):
        return False


def get_llm(
    model_name: Optional[str] = None,
    temperature: Optional[float] = None,
    max_output_tokens: Optional[int] = None,
    max_retries: int = 2,
    api_key: Optional[str] = None,
    verbose: bool = True
) -> BaseChatModel:
    """
    Cr√©e une instance LLM en essayant Ollama en premier, puis fallback vers Gemini
    
    Args:
        model_name: Nom du mod√®le (si None, utilise Config.MODEL_NAME)
        temperature: Temp√©rature du mod√®le (si None, utilise Config.TEMPERATURE)
        max_output_tokens: Nombre max de tokens de sortie (optionnel)
        max_retries: Nombre de tentatives en cas d'erreur
        api_key: Cl√© API Google (optionnel, pour Gemini)
        verbose: Si True, affiche des messages informatifs
        
    Returns:
        Instance de BaseChatModel (ChatOllama ou ChatGoogleGenerativeAI)
        
    Raises:
        ValueError: Si ni Ollama ni Gemini ne sont disponibles
    """
    # Utiliser les valeurs par d√©faut de la config si non sp√©cifi√©es
    if model_name is None:
        model_name = Config.MODEL_NAME
    if temperature is None:
        temperature = Config.TEMPERATURE
    
    # V√©rifier si on force l'utilisation d'un provider sp√©cifique
    use_ollama = os.getenv("USE_OLLAMA", "").lower()
    use_gemini = os.getenv("USE_GEMINI", "").lower()
    
    # Si USE_OLLAMA est explicitement d√©fini √† "true", utiliser Ollama
    if use_ollama == "true":
        if verbose:
            print("üîß Mode Ollama forc√© via USE_OLLAMA=true")
        return _create_ollama_llm(
            model_name=model_name,
            temperature=temperature,
            max_output_tokens=max_output_tokens,
            max_retries=max_retries,
            verbose=verbose
        )
    
    # Si USE_GEMINI est explicitement d√©fini √† "true", utiliser Gemini
    if use_gemini == "true":
        if verbose:
            print("üîß Mode Gemini forc√© via USE_GEMINI=true")
        return _create_gemini_llm(
            model_name=model_name,
            temperature=temperature,
            max_output_tokens=max_output_tokens,
            max_retries=max_retries,
            api_key=api_key,
            verbose=verbose
        )
    
    # Sinon, essayer Ollama en premier (par d√©faut)
    if verbose:
        print("üîç V√©rification de la disponibilit√© d'Ollama...")
    
    if _test_ollama_connection():
        if verbose:
            print(f"‚úÖ Ollama d√©tect√© ! Utilisation du mod√®le local: {Config.OLLAMA_MODEL_NAME}")
        return _create_ollama_llm(
            model_name=Config.OLLAMA_MODEL_NAME,
            temperature=temperature,
            max_output_tokens=max_output_tokens,
            max_retries=max_retries,
            verbose=verbose
        )
    else:
        # Fallback vers Gemini
        if verbose:
            print("‚ö†Ô∏è Ollama non disponible, utilisation de Gemini en fallback...")
        return _create_gemini_llm(
            model_name=model_name,
            temperature=temperature,
            max_output_tokens=max_output_tokens,
            max_retries=max_retries,
            api_key=api_key,
            verbose=verbose
        )


def _create_ollama_llm(
    model_name: str,
    temperature: float,
    max_output_tokens: Optional[int],
    max_retries: int,
    verbose: bool
) -> BaseChatModel:
    """Cr√©e une instance ChatOllama"""
    try:
        from langchain_ollama import ChatOllama
    except ImportError:
        raise ImportError(
            "langchain-ollama n'est pas install√©. "
            "Installez-le avec: pip install langchain-ollama"
        )
    
    llm_kwargs = {
        "model": model_name,
        "temperature": temperature,
        "base_url": Config.OLLAMA_BASE_URL,
    }
    
    # max_output_tokens n'est pas support√© par ChatOllama de la m√™me mani√®re
    # On peut utiliser num_predict √† la place si n√©cessaire
    if max_output_tokens:
        llm_kwargs["num_predict"] = max_output_tokens
    
    if verbose:
        print(f"ü§ñ Initialisation de ChatOllama avec le mod√®le: {model_name}")
    
    return ChatOllama(**llm_kwargs)


def _create_gemini_llm(
    model_name: str,
    temperature: float,
    max_output_tokens: Optional[int],
    max_retries: int,
    api_key: Optional[str],
    verbose: bool
) -> BaseChatModel:
    """Cr√©e une instance ChatGoogleGenerativeAI"""
    try:
        from langchain_google_genai import ChatGoogleGenerativeAI
    except ImportError:
        raise ImportError(
            "langchain-google-genai n'est pas install√©. "
            "Installez-le avec: pip install langchain-google-genai"
        )
    
    # Configuration de la cl√© API
    if api_key:
        os.environ["GOOGLE_API_KEY"] = api_key
    elif "GOOGLE_API_KEY" not in os.environ:
        raise ValueError(
            "Cl√© API Google manquante pour Gemini. "
            "D√©finissez-la via GOOGLE_API_KEY dans .env ou passez-la en param√®tre. "
            "Ou installez et d√©marrez Ollama pour utiliser un LLM local."
        )
    
    llm_kwargs = {
        "model": model_name,
        "temperature": temperature,
        "max_retries": max_retries,
    }
    
    if max_output_tokens:
        llm_kwargs["max_output_tokens"] = max_output_tokens
    
    if verbose:
        print(f"ü§ñ Initialisation de ChatGoogleGenerativeAI avec le mod√®le: {model_name}")
    
    return ChatGoogleGenerativeAI(**llm_kwargs)



